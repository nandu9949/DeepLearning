{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dddeae04",
      "metadata": {
        "id": "dddeae04"
      },
      "source": [
        "# Real-world data representation using tensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aoZRAbK_4tl",
        "outputId": "a3cf3e6e-3258-4c95-8445-0946b89a0195"
      },
      "id": "3aoZRAbK_4tl",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4212b6f",
      "metadata": {
        "id": "a4212b6f"
      },
      "source": [
        "Tensors are the building blocks for data in PyTorch. Neural networks take tensors as input and produce tensors as outputs. In fact, all operations within a neural network and during optimization are operations between tensors, and all parameters (for example, weights and biases) in a neural network are tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f046ba2",
      "metadata": {
        "id": "2f046ba2"
      },
      "source": [
        "# Working with images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee967fb6",
      "metadata": {
        "id": "ee967fb6"
      },
      "source": [
        "An image is represented as a collection of scalars arranged in a regular grid with a\n",
        "height and a width (in pixels)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd8d972",
      "metadata": {
        "id": "3fd8d972"
      },
      "source": [
        "### Adding color channels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4339b2",
      "metadata": {
        "id": "3a4339b2"
      },
      "source": [
        "### Loading an image\n",
        "Images come in several different file formats, but luckily there are plenty of ways to\n",
        "load images in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dee779e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dee779e8",
        "outputId": "8bcaadb3-4907-4e70-a02e-3d7a2900f013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imageio) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio) (11.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "59b8cd50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59b8cd50",
        "outputId": "542a224d-3adb-44d3-dcd5-6f0ae82f2cee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(720, 1280, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import imageio.v2 as imageio\n",
        "\n",
        "img_arr = imageio.imread('/content/drive/MyDrive/dataset/p1ch4/image-dog/bobby.jpg')\n",
        "img_arr.shape\n",
        "\n",
        "#This code loads a .jpg image using the imageio library and checks its shape. The resulting image is a NumPy array with height, width, and RGB channels."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting Image to PyTorch Tensor and Permuting Dimensions**"
      ],
      "metadata": {
        "id": "LyqazJpBA31J"
      },
      "id": "LyqazJpBA31J"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2cadaeaf",
      "metadata": {
        "id": "2cadaeaf"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "527e5449",
      "metadata": {
        "id": "527e5449"
      },
      "outputs": [],
      "source": [
        "img = torch.from_numpy(img_arr)\n",
        "out = img.permute(2, 0, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preallocating a Tensor for a Batch of Images"
      ],
      "metadata": {
        "id": "TlZQCGWyBTHO"
      },
      "id": "TlZQCGWyBTHO"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "78f68bca",
      "metadata": {
        "id": "78f68bca"
      },
      "outputs": [],
      "source": [
        "batch_size = 3\n",
        "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)\n",
        "#Before loading multiple images, a tensor of shape (batch_size, channels, height, width) is created to store them efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83e2b9cd",
      "metadata": {
        "id": "83e2b9cd"
      },
      "source": [
        "Loading Multiple PNG Images into a Batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "77f8a441",
      "metadata": {
        "id": "77f8a441"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/dataset/p1ch4/image-cats'\n",
        "filenames = [name for name in os.listdir(data_dir) if name.endswith('.png')]\n",
        "\n",
        "for i, filename in enumerate(filenames):\n",
        "    img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
        "    img_t = torch.from_numpy(img_arr).permute(2, 0, 1)[:3]  # Keep only RGB\n",
        "    batch[i] = img_t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing Pixel Values to [0, 1]"
      ],
      "metadata": {
        "id": "kABLgDivB0lZ"
      },
      "id": "kABLgDivB0lZ"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "365aba78",
      "metadata": {
        "id": "365aba78"
      },
      "outputs": [],
      "source": [
        "batch = batch.float()\n",
        "batch /= 255.0\n",
        "#Convert the pixel values from uint8 to float and scale them to the range [0, 1] by dividing by 255."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff766681",
      "metadata": {
        "id": "ff766681"
      },
      "source": [
        "Standardizing Each Channel (Zero Mean, Unit Std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7a37646c",
      "metadata": {
        "id": "7a37646c"
      },
      "outputs": [],
      "source": [
        "n_channels = batch.shape[1]\n",
        "\n",
        "for c in range(n_channels):\n",
        "    mean = torch.mean(batch[:, c])\n",
        "    std = torch.std(batch[:, c])\n",
        "    batch[:, c] = (batch[:, c] - mean) / std\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ab6369",
      "metadata": {
        "id": "e1ab6369"
      },
      "source": [
        "* We can perform several other operations on inputs, such as geometric transformations\n",
        "like rotations, scaling, and cropping."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6df581e",
      "metadata": {
        "id": "e6df581e"
      },
      "source": [
        "## 3D images: Volumetric data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "516ad8db",
      "metadata": {
        "id": "516ad8db"
      },
      "source": [
        "Volumetric data such as CT scans are represented as 3D tensors where each slice corresponds to a cross-section of the subject's body. CT scans typically contain only a single intensity channel (grayscale), and we stack these slices into a tensor with shape [C, D, H, W]."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20181bac",
      "metadata": {
        "id": "20181bac"
      },
      "source": [
        " Loading a Volumetric CT Scan Using imageio.volread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "06802fd8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06802fd8",
        "outputId": "936c8071-0b80-4d4b-8b4d-6253e2b20a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading DICOM (examining files): 1/99 files (1.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/99 files (2.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3/99 files (3.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4/99 files (4.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/99 files (5.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6/99 files (6.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/99 files (7.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/99 files (8.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9/99 files (9.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/99 files (10.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/99 files (11.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/99 files (12.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/99 files (13.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/99 files (14.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/99 files (15.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/99 files (16.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/99 files (17.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/99 files (18.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/99 files (19.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/99 files (20.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/99 files (21.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/99 files (22.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23/99 files (23.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/99 files (24.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/99 files (25.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/99 files (26.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/99 files (27.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/99 files (28.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/99 files (29.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/99 files (30.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/99 files (31.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/99 files (32.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b33/99 files (33.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34/99 files (34.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35/99 files (35.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/99 files (36.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37/99 files (37.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38/99 files (38.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39/99 files (39.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/99 files (40.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41/99 files (41.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42/99 files (42.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43/99 files (43.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44/99 files (44.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45/99 files (45.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b47/99 files (47.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b48/99 files (48.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b49/99 files (49.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b50/99 files (50.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/99 files (51.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/99 files (52.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b53/99 files (53.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b54/99 files (54.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b55/99 files (55.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b56/99 files (56.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57/99 files (57.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b58/99 files (58.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b59/99 files (59.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b60/99 files (60.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b61/99 files (61.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b62/99 files (62.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/99 files (63.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b64/99 files (64.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b65/99 files (65.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b66/99 files (66.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b67/99 files (67.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b68/99 files (68.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b69/99 files (69.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b70/99 files (70.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b71/99 files (71.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b72/99 files (72.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b73/99 files (73.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b74/99 files (74.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b75/99 files (75.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b76/99 files (76.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b77/99 files (77.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b78/99 files (78.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b79/99 files (79.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b80/99 files (80.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b81/99 files (81.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b82/99 files (82.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b83/99 files (83.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b84/99 files (84.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b85/99 files (85.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b86/99 files (86.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b87/99 files (87.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b88/99 files (88.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b89/99 files (89.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b90/99 files (90.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b91/99 files (91.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b92/99 files (92.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b93/99 files (93.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b94/99 files (94.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b95/99 files (96.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b96/99 files (97.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b97/99 files (98.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b98/99 files (99.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99 files (100.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99 files (100.0%)\n",
            "  Found 1 correct series.\n",
            "Reading DICOM (loading data): 2/99  (2.0%)\b\b\b\b\b\b\b\b\b\b\b\b4/99  (4.0%)\b\b\b\b\b\b\b\b\b\b\b\b6/99  (6.1%)\b\b\b\b\b\b\b\b\b\b\b\b7/99  (7.1%)\b\b\b\b\b\b\b\b\b\b\b\b8/99  (8.1%)\b\b\b\b\b\b\b\b\b\b\b\b10/99  (10.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/99  (11.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/99  (12.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/99  (14.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/99  (15.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/99  (16.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/99  (17.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/99  (20.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/99  (21.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/99  (22.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b23/99  (23.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/99  (24.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/99  (25.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/99  (28.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/99  (29.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/99  (30.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/99  (31.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/99  (32.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b33/99  (33.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b34/99  (34.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/99  (36.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b37/99  (37.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b38/99  (38.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/99  (40.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b41/99  (41.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b42/99  (42.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b43/99  (43.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b45/99  (45.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b46/99  (46.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b47/99  (47.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b48/99  (48.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b49/99  (49.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b50/99  (50.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/99  (51.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/99  (52.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b53/99  (53.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b54/99  (54.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b58/99  (58.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b59/99  (59.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b61/99  (61.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b66/99  (66.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b67/99  (67.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b68/99  (68.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b69/99  (69.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b71/99  (71.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b74/99  (74.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b75/99  (75.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b79/99  (79.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b80/99  (80.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b81/99  (81.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b83/99  (83.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b84/99  (84.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b87/99  (87.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b88/99  (88.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b89/99  (89.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b91/99  (91.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b92/99  (92.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b94/99  (94.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b95/99  (96.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b98/99  (99.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99  (100.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99  (100.0%)\n",
            "(99, 512, 512)\n"
          ]
        }
      ],
      "source": [
        "import imageio\n",
        "dir_path = \"/content/drive/MyDrive/dataset/p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083\"\n",
        "vol_arr = imageio.volread(dir_path, 'DICOM')\n",
        "print(vol_arr.shape) #\n",
        "#CT scans are stored as multiple DICOM files. Using imageio.volread, we can load these into a 3D NumPy array of shape [depth, height, width]."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding a Channel Dimension"
      ],
      "metadata": {
        "id": "zKIpXuPpDHTo"
      },
      "id": "zKIpXuPpDHTo"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "443e9ad6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "443e9ad6",
        "outputId": "c3f0ee79-3feb-4c5a-aee4-842687d530ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 99, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "vol = torch.from_numpy(vol_arr).float()\n",
        "vol = torch.unsqueeze(vol, 0)\n",
        "print(vol.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "293d6cdc",
      "metadata": {
        "id": "293d6cdc"
      },
      "source": [
        "## Representing tabular data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27b86d33",
      "metadata": {
        "id": "27b86d33"
      },
      "source": [
        "Tabular data, such as CSV files or spreadsheets, is one of the most common data forms in real-world ML problems. Each row represents one sample; columns store features (numerical or categorical). However, unlike tables, PyTorch tensors must be homogeneous (same data type), typically float32, to work with neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a9e886c",
      "metadata": {
        "id": "9a9e886c"
      },
      "source": [
        "Loading Wine Data as a Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3a01510c",
      "metadata": {
        "id": "3a01510c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "wine_path = \"/content/drive/MyDrive/dataset/p1ch4/tabular-wine/winequality-white.csv\"\n",
        "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
        "# loaded the CSV file using NumPy’s loadtxt and convert it to a PyTorch tensor. This avoids loading unnecessary libraries like Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "01bd0e85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01bd0e85",
        "outputId": "896e5f13-030f-48b7-ad5f-f1a04c152cad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4898, 12)\n",
            "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "col_list = next(csv.reader(open(wine_path), delimiter=';'))\n",
        "print(wineq_numpy.shape)  # Output: (4898, 12)\n",
        "print(col_list)  # List of column names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f1a84273",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1a84273",
        "outputId": "84240f13-24f8-4c50-9c72-91eb13f01221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4898, 12]) torch.float32\n"
          ]
        }
      ],
      "source": [
        "#to convert the NumPy array to a PyTorch tensor\n",
        "import torch\n",
        "\n",
        "wineq = torch.from_numpy(wineq_numpy)\n",
        "print(wineq.shape, wineq.dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3de2a324",
      "metadata": {
        "id": "3de2a324"
      },
      "source": [
        "### Representing scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4f0e220c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f0e220c",
        "outputId": "9274a53d-5926-43d8-ca67-88d1eaf550f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 7.0000,  0.2700,  0.3600,  ...,  3.0000,  0.4500,  8.8000],\n",
              "         [ 6.3000,  0.3000,  0.3400,  ...,  3.3000,  0.4900,  9.5000],\n",
              "         [ 8.1000,  0.2800,  0.4000,  ...,  3.2600,  0.4400, 10.1000],\n",
              "         ...,\n",
              "         [ 6.5000,  0.2400,  0.1900,  ...,  2.9900,  0.4600,  9.4000],\n",
              "         [ 5.5000,  0.2900,  0.3000,  ...,  3.3400,  0.3800, 12.8000],\n",
              "         [ 6.0000,  0.2100,  0.3800,  ...,  3.2600,  0.3200, 11.8000]]),\n",
              " torch.Size([4898, 11]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "data = wineq[:, :-1]\n",
        "data, data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "dd76639a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd76639a",
        "outputId": "34785954-f63c-43b8-cff1-f8d9640e4847"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([6., 6., 6.,  ..., 6., 7., 6.]), torch.Size([4898]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "target = wineq[:, -1]\n",
        "target, target.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9aa2834",
      "metadata": {
        "id": "b9aa2834"
      },
      "source": [
        "If we want to transform the target tensor in a tensor of labels, we have two options,\n",
        "depending on the strategy or what we use the categorical data for.\n",
        "One is simply to treat labels as an integer vector of scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c64d0a0",
      "metadata": {
        "id": "5c64d0a0",
        "outputId": "1d847b37-61a3-45ff-b006-75c3829dca29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([6, 6, 6,  ..., 6, 7, 6])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target = wineq[:, -1].long()\n",
        "target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fc5c8d4",
      "metadata": {
        "id": "6fc5c8d4"
      },
      "source": [
        "### One-hot encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cdf3cac",
      "metadata": {
        "id": "5cdf3cac"
      },
      "source": [
        "For classification, target scores (like 0–10) can be:\n",
        "\n",
        "Stored as integer class labels\n",
        "\n",
        "Or one-hot encoded: a vector with all zeros and one “1” at the label index.\n",
        "\n",
        "This is useful for models that expect categorical input/output in vector form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e31882ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e31882ce",
        "outputId": "fe1c9b67-80d2-4d3b-a502-6a2e1bdf7269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6, 6, 6, 6, 6])\n"
          ]
        }
      ],
      "source": [
        "target = wineq[:, -1].long()  # Convert float target to integer class labels\n",
        "print(target[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "aad4e0f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aad4e0f9",
        "outputId": "98e0a068-1865-43fe-9cc8-4371416f0082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4898, 10])\n"
          ]
        }
      ],
      "source": [
        "# Create zero tensor with shape (4898, 10) — 10 classes (scores 0 to 9)\n",
        "target_onehot = torch.zeros(target.shape[0], 10)\n",
        "# One-hot encode using scatter_\n",
        "target_onehot.scatter_(1, target.unsqueeze(1), 1.0)\n",
        "print(target_onehot.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ce06e2d",
      "metadata": {
        "id": "4ce06e2d"
      },
      "source": [
        "### When to categorize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "778a826f",
      "metadata": {
        "id": "778a826f"
      },
      "source": [
        "To help the  model learn better, normalizing features:\n",
        "Subtract the mean and divide by the standard deviation for each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "db93acb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db93acb6",
        "outputId": "cea3d160-e192-4030-bd0b-0bf919611155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6.8548e+00, 2.7824e-01, 3.3419e-01, 6.3914e+00, 4.5772e-02, 3.5308e+01,\n",
            "        1.3836e+02, 9.9403e-01, 3.1883e+00, 4.8985e-01, 1.0514e+01])\n",
            "tensor([7.1211e-01, 1.0160e-02, 1.4646e-02, 2.5726e+01, 4.7733e-04, 2.8924e+02,\n",
            "        1.8061e+03, 8.9455e-06, 2.2801e-02, 1.3025e-02, 1.5144e+00])\n"
          ]
        }
      ],
      "source": [
        "data_mean = torch.mean(data, dim=0)\n",
        "data_var = torch.var(data, dim=0)\n",
        "\n",
        "print(data_mean)  # Mean for each of the 11 features\n",
        "print(data_var)   # Variance for each of the 11 features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f5b0bd76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5b0bd76",
        "outputId": "e696486f-7f3c-4538-e6be-66dd873516ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7.1211e-01, 1.0160e-02, 1.4646e-02, 2.5726e+01, 4.7733e-04, 2.8924e+02,\n",
              "        1.8061e+03, 8.9455e-06, 2.2801e-02, 1.3025e-02, 1.5144e+00])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "data_var = torch.var(data, dim=0)\n",
        "data_var"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize Data"
      ],
      "metadata": {
        "id": "QR6vRyJJHOTB"
      },
      "id": "QR6vRyJJHOTB"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "5b70a9c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b70a9c8",
        "outputId": "ec9fe429-fdc0-4e80-c706-0b3bbbeec962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.7208e-01, -8.1761e-02,  2.1326e-01,  2.8211e+00, -3.5351e-02,\n",
            "          5.6987e-01,  7.4449e-01,  2.3313e+00, -1.2468e+00, -3.4915e-01,\n",
            "         -1.3930e+00],\n",
            "        [-6.5743e-01,  2.1587e-01,  4.7996e-02, -9.4467e-01,  1.4773e-01,\n",
            "         -1.2529e+00, -1.4967e-01, -9.1472e-03,  7.3995e-01,  1.3422e-03,\n",
            "         -8.2419e-01],\n",
            "        [ 1.4756e+00,  1.7450e-02,  5.4378e-01,  1.0027e-01,  1.9350e-01,\n",
            "         -3.1211e-01, -9.7324e-01,  3.5864e-01,  4.7505e-01, -4.3677e-01,\n",
            "         -3.3663e-01]])\n"
          ]
        }
      ],
      "source": [
        "data_normalized = (data - data_mean) / torch.sqrt(data_var)\n",
        "\n",
        "print(data_normalized[:3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7db4a6f2",
      "metadata": {
        "id": "7db4a6f2"
      },
      "source": [
        "### Finding thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87cd9bb8",
      "metadata": {
        "id": "87cd9bb8"
      },
      "source": [
        "Find a simple rule (threshold) to distinguish good from bad wines using just one feature. We'll use total sulfur dioxide as a test feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "d6886cca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6886cca",
        "outputId": "04836f66-6b95-44a4-9a37-bfbbd04e630d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4898]) torch.bool tensor(20)\n"
          ]
        }
      ],
      "source": [
        "bad_indexes = target <= 3\n",
        "print(bad_indexes.shape, bad_indexes.dtype, bad_indexes.sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Use Boolean indexing to extract bad wines"
      ],
      "metadata": {
        "id": "z1bM-tX1IJHE"
      },
      "id": "z1bM-tX1IJHE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b593ca",
      "metadata": {
        "id": "b9b593ca",
        "outputId": "e458e26e-39b2-4a57-a3bb-8a6b33838af7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([20, 11])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bad_data = data[bad_indexes]\n",
        "print(bad_data.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c7b5c27",
      "metadata": {
        "id": "0c7b5c27"
      },
      "source": [
        "Split wine data into three quality levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "581d9bb2",
      "metadata": {
        "id": "581d9bb2"
      },
      "outputs": [],
      "source": [
        "bad_data = data[target <= 3]\n",
        "mid_data = data[(target > 3) & (target < 7)]\n",
        "good_data = data[target >= 7]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c9fd19",
      "metadata": {
        "id": "21c9fd19"
      },
      "source": [
        "Compute average feature values for each group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "78a43e71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78a43e71",
        "outputId": "a5bdbdd5-a48d-4ec0-c57e-b8c566918dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0 fixed acidity          7.60   6.89   6.73\n",
            " 1 volatile acidity       0.33   0.28   0.27\n",
            " 2 citric acid            0.34   0.34   0.33\n",
            " 3 residual sugar         6.39   6.71   5.26\n",
            " 4 chlorides              0.05   0.05   0.04\n",
            " 5 free sulfur dioxide   53.33  35.42  34.55\n",
            " 6 total sulfur dioxide 170.60 141.83 125.25\n",
            " 7 density                0.99   0.99   0.99\n",
            " 8 pH                     3.19   3.18   3.22\n",
            " 9 sulphates              0.47   0.49   0.50\n",
            "10 alcohol               10.34  10.26  11.42\n"
          ]
        }
      ],
      "source": [
        "bad_mean = torch.mean(bad_data, dim=0)\n",
        "mid_mean = torch.mean(mid_data, dim=0)\n",
        "good_mean = torch.mean(good_data, dim=0)\n",
        "\n",
        "for i, args in enumerate(zip(col_list, bad_mean, mid_mean, good_mean)):\n",
        "    print('{:2} {:20} {:6.2f} {:6.2f} {:6.2f}'.format(i, *args))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3e1b8ae",
      "metadata": {
        "id": "a3e1b8ae"
      },
      "source": [
        "Use a sulfur dioxide threshold to predict good wines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_sulfur_threshold = 141.83\n",
        "total_sulfur_data = data[:, 6]\n",
        "\n",
        "predicted_indexes = total_sulfur_data < total_sulfur_threshold\n",
        "print(predicted_indexes.shape, predicted_indexes.dtype, predicted_indexes.sum())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYwl9dBSIwRi",
        "outputId": "c9990493-c402-4f05-96d7-447fd0f9fec3"
      },
      "id": "uYwl9dBSIwRi",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4898]) torch.bool tensor(2727)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the actual good wine indexes\n",
        "python\n",
        "Copy\n",
        "Edit\n"
      ],
      "metadata": {
        "id": "I42K9rmpIyQr"
      },
      "id": "I42K9rmpIyQr"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "809998a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "809998a7",
        "outputId": "2bab0e8f-f435-4b64-ce24-90909e77f272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4898]) torch.bool tensor(3258)\n"
          ]
        }
      ],
      "source": [
        "actual_indexes = target > 5\n",
        "print(actual_indexes.shape, actual_indexes.dtype, actual_indexes.sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04b7a6c7",
      "metadata": {
        "id": "04b7a6c7"
      },
      "source": [
        "Evaluate prediction quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "c40b572d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c40b572d",
        "outputId": "0325e016-ee4d-463d-fb2c-d1698957e2d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018 0.74000733406674 0.6193984039287906\n"
          ]
        }
      ],
      "source": [
        "n_matches = torch.sum(actual_indexes & predicted_indexes).item()\n",
        "n_predicted = torch.sum(predicted_indexes).item()\n",
        "n_actual = torch.sum(actual_indexes).item()\n",
        "\n",
        "print(n_matches, n_matches / n_predicted, n_matches / n_actual)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef84277b",
      "metadata": {
        "id": "ef84277b"
      },
      "source": [
        "## Working with time series"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a529e7f",
      "metadata": {
        "id": "3a529e7f"
      },
      "source": [
        "### Adding a time dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e04ab161",
      "metadata": {
        "id": "e04ab161"
      },
      "source": [
        "For every hour, the dataset reports the following variables:\n",
        "- Index of record: instant\n",
        "- Day of month: day\n",
        "- Season: season (1: spring, 2: summer, 3: fall, 4: winter)\n",
        "- Year: yr (0: 2011, 1: 2012)\n",
        "- Month: mnth (1 to 12)\n",
        "- Hour: hr (0 to 23)\n",
        "- Holiday status: holiday\n",
        "- Day of the week: weekday\n",
        "- Working day status: workingday\n",
        "- Weather situation: weathersit (1: clear, 2:mist, 3: light rain/snow, 4: heavy\n",
        "rain/snow)\n",
        "- Temperature in °C: temp\n",
        "- Perceived temperature in °C: atemp\n",
        "- Humidity: hum\n",
        "- Wind speed: windspeed\n",
        "- Number of casual users: casual\n",
        "- Number of registered users: registered\n",
        "- Count of rental bikes: cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c635d0f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c635d0f5",
        "outputId": "f0b4b00c-5b41-4dc4-d9ea-1b98df59f808"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.0000e+00, 1.3000e+01,\n",
              "         1.6000e+01],\n",
              "        [2.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 8.0000e+00, 3.2000e+01,\n",
              "         4.0000e+01],\n",
              "        [3.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 5.0000e+00, 2.7000e+01,\n",
              "         3.2000e+01],\n",
              "        ...,\n",
              "        [1.7377e+04, 3.1000e+01, 1.0000e+00,  ..., 7.0000e+00, 8.3000e+01,\n",
              "         9.0000e+01],\n",
              "        [1.7378e+04, 3.1000e+01, 1.0000e+00,  ..., 1.3000e+01, 4.8000e+01,\n",
              "         6.1000e+01],\n",
              "        [1.7379e+04, 3.1000e+01, 1.0000e+00,  ..., 1.2000e+01, 3.7000e+01,\n",
              "         4.9000e+01]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "bikes_numpy = np.loadtxt(\n",
        "\"/content/drive/MyDrive/dataset/p1ch4/bike-sharing-dataset/hour-fixed.csv\",\n",
        "    dtype=np.float32,\n",
        "    delimiter=\",\",\n",
        "    skiprows=1,\n",
        "    converters={1: lambda x: float(x[8:10])})\n",
        "bikes = torch.from_numpy(bikes_numpy)\n",
        "bikes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b7a2a9a",
      "metadata": {
        "id": "6b7a2a9a"
      },
      "source": [
        "### Shaping the data by time period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "8e82cd43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e82cd43",
        "outputId": "2420811a-7db8-4d7a-cf2a-66b637696e7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([17520, 17]), (17, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "bikes.shape, bikes.stride()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb2493b",
      "metadata": {
        "id": "1eb2493b"
      },
      "source": [
        "That’s 17,520 hours, 17 columns. Now let’s reshape the data to have 3 axes—day, hour,\n",
        "and then our 17 columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "0610c5fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0610c5fa",
        "outputId": "91de12e0-e0ff-4c7a-a84f-226d804c0f4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([730, 24, 17]), (408, 17, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "daily_bikes = bikes.view(-1, 24, bikes.shape[1])\n",
        "daily_bikes.shape, daily_bikes.stride()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81eab7d6",
      "metadata": {
        "id": "81eab7d6"
      },
      "source": [
        "We see that the rightmost dimension is the number of columns in the original\n",
        "dataset. Then, in the middle dimension, we have time, split into chunks of 24 sequential\n",
        "hours. In other words, we now have N sequences of L hours in a day, for C channels.\n",
        "To get to our desired N × C × L ordering, we need to transpose the tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "ed7e9f7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed7e9f7f",
        "outputId": "8a41257c-3e94-45a9-8bb1-2d8103af2189"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([730, 17, 24]), (408, 1, 17))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "daily_bikes = daily_bikes.transpose(1, 2)\n",
        "daily_bikes.shape, daily_bikes.stride()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9be12b0",
      "metadata": {
        "id": "f9be12b0"
      },
      "source": [
        "In order to make it easier to render our data, we’re going to limit ourselves to the\n",
        "first day for a moment. We initialize a zero-filled matrix with a number of rows equal\n",
        "to the number of hours in the day and number of columns equal to the number of\n",
        "weather levels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "fae7ee4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fae7ee4a",
        "outputId": "38cb8e59-49ec-41d5-891f-20102ccb58e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "first_day = bikes[:24].long()\n",
        "weather_onehot = torch.zeros(first_day.shape[0], 4)\n",
        "first_day[:,9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "ce1520ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce1520ef",
        "outputId": "30f8398c-1c45-40a0-8a5b-8daae554757e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "weather_onehot.scatter_(\n",
        "    dim=1,\n",
        "    index=first_day[:,9].unsqueeze(1).long() - 1,\n",
        "    value=1.0)\n",
        "\n",
        "#Decreases the values by 1 because weather situation ranges from 1 to 4, while indices are 0-based"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "641409cd",
      "metadata": {
        "id": "641409cd"
      },
      "source": [
        "Last, we concatenate our matrix to our original dataset using the cat function.\n",
        "Let’s look at the first of our results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "39b7a086",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39b7a086",
        "outputId": "8c4fd71e-4663-401e-81c4-7c622f4728b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0000,  1.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  6.0000,\n",
              "          0.0000,  1.0000,  0.2400,  0.2879,  0.8100,  0.0000,  3.0000, 13.0000,\n",
              "         16.0000,  1.0000,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "torch.cat((bikes[:24], weather_onehot), 1)[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "a23bf925",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a23bf925",
        "outputId": "0b41f97d-e073-4618-b1e5-88beb454ce08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([730, 4, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "daily_weather_onehot = torch.zeros(daily_bikes.shape[0], 4,\n",
        "daily_bikes.shape[2])\n",
        "daily_weather_onehot.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "1c3b4a8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c3b4a8d",
        "outputId": "412e85a5-3109-4491-9d91-5ac21d1ba393"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([730, 4, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "daily_weather_onehot.scatter_(\n",
        "1, daily_bikes[:,9,:].long().unsqueeze(1) - 1, 1.0)\n",
        "daily_weather_onehot.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "af9e8d34",
      "metadata": {
        "id": "af9e8d34"
      },
      "outputs": [],
      "source": [
        "daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "58829990",
      "metadata": {
        "id": "58829990"
      },
      "outputs": [],
      "source": [
        "daily_bikes[:, 9, :] = (daily_bikes[:, 9, :] - 1.0) / 3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f01b59a8",
      "metadata": {
        "id": "f01b59a8"
      },
      "source": [
        "There are multiple possibilities for rescaling variables. We can either map their\n",
        "range to [0.0, 1.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "c3dae28f",
      "metadata": {
        "id": "c3dae28f"
      },
      "outputs": [],
      "source": [
        "temp = daily_bikes[:, 10, :]\n",
        "temp_min = torch.min(temp)\n",
        "temp_max = torch.max(temp)\n",
        "daily_bikes[:, 10, :] = ((daily_bikes[:, 10, :] - temp_min)\n",
        "/ (temp_max - temp_min))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "225fa670",
      "metadata": {
        "id": "225fa670"
      },
      "source": [
        "or subtract the mean and divide by the standard deviation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "84dacec8",
      "metadata": {
        "id": "84dacec8"
      },
      "outputs": [],
      "source": [
        "temp = daily_bikes[:, 10, :]\n",
        "daily_bikes[:, 10, :] = ((daily_bikes[:, 10,:] - torch.mean(temp))\n",
        "/ torch.std(temp))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2bc059c",
      "metadata": {
        "id": "c2bc059c"
      },
      "source": [
        "In the latter case, our variable will have 0 mean and unitary standard deviation. If our\n",
        "variable were drawn from a Gaussian distribution, 68% of the samples would sit in the\n",
        "[-1.0, 1.0] interval."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aaaece9",
      "metadata": {
        "id": "9aaaece9"
      },
      "source": [
        "## Representing text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2416914",
      "metadata": {
        "id": "e2416914"
      },
      "source": [
        "Deep learning models process text by converting it into tensors. This section introduces one-hot encoding at the character and word level, and explains how to turn raw text into tensor data suitable for models like RNNs or Transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "876f1ca5",
      "metadata": {
        "id": "876f1ca5"
      },
      "source": [
        "Our goal in this section is to turn text into something a neural network can process:\n",
        "a tensor of numbers, just like our previous cases. If we can do that and later\n",
        "choose the right architecture for our text-processing job, we’ll be in the position of\n",
        "doing NLP with PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e82305df",
      "metadata": {
        "id": "e82305df"
      },
      "source": [
        "### Converting text to numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e918f9c7",
      "metadata": {
        "id": "e918f9c7"
      },
      "source": [
        "Goal: Turn raw text into a tensor format.\n",
        "\n",
        "Two main levels of processing:\n",
        "\n",
        "Character-level (each character encoded)\n",
        "\n",
        "Word-level (each word encoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "eff3f659",
      "metadata": {
        "id": "eff3f659"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/dataset/p1ch4/jane-austen/1342-0.txt', encoding='utf8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6aafe9b",
      "metadata": {
        "id": "d6aafe9b"
      },
      "source": [
        "### One-hot-encoding characters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8e3bf88",
      "metadata": {
        "id": "d8e3bf88"
      },
      "source": [
        "One-Hot Encoding is a method of representing characters or words by a vector where only one element is set to one and all others are zero, based on their position in the vocabulary. This results in a sparse, semantically independent vector with a high dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97250bc1",
      "metadata": {
        "id": "97250bc1"
      },
      "source": [
        "Pick a line of text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "d6ceb5b3",
      "metadata": {
        "id": "d6ceb5b3"
      },
      "outputs": [],
      "source": [
        "lines = text.split('\\n')\n",
        "line = lines[200]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59400cd8",
      "metadata": {
        "id": "59400cd8"
      },
      "source": [
        "Create one-hot encoding tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "d28cac98",
      "metadata": {
        "id": "d28cac98"
      },
      "outputs": [],
      "source": [
        "letter_t = torch.zeros(len(line), 128)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c2c8f77",
      "metadata": {
        "id": "6c2c8f77"
      },
      "source": [
        "Fill tensor using ASCII codes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "9cfc802e",
      "metadata": {
        "id": "9cfc802e"
      },
      "outputs": [],
      "source": [
        "for i, letter in enumerate(line.lower().strip()):\n",
        "    letter_index = ord(letter) if ord(letter) < 128 else 0\n",
        "    letter_t[i][letter_index] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e09d79d",
      "metadata": {
        "id": "2e09d79d"
      },
      "source": [
        "### One-hot encoding whole words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a43a04",
      "metadata": {
        "id": "39a43a04"
      },
      "source": [
        "Clean the words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "2b29dc11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b29dc11",
        "outputId": "17b2bfd5-5812-4fe5-8224-d8651596296b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('“Impossible, Mr. Bennet, impossible, when I am not acquainted with him',\n",
              " ['impossible',\n",
              "  'mr',\n",
              "  'bennet',\n",
              "  'impossible',\n",
              "  'when',\n",
              "  'i',\n",
              "  'am',\n",
              "  'not',\n",
              "  'acquainted',\n",
              "  'with',\n",
              "  'him'])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "def clean_words(input_str):\n",
        "    punctuation = '.,;:\"!?”“_-'\n",
        "    word_list = input_str.lower().replace('\\n',' ').split()\n",
        "    word_list = [word.strip(punctuation) for word in word_list]\n",
        "    return word_list\n",
        "words_in_line = clean_words(line)\n",
        "line, words_in_line"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6fbcb45",
      "metadata": {
        "id": "e6fbcb45"
      },
      "source": [
        "Create vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "960409c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "960409c0",
        "outputId": "7ca3c6bb-174e-429e-d40e-a7ab88a6d3b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7261, 3394)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "word_list = sorted(set(clean_words(text)))\n",
        "word2index_dict = {word: i for (i, word) in enumerate(word_list)}\n",
        "len(word2index_dict), word2index_dict['impossible']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "8bcf9cd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bcf9cd5",
        "outputId": "4ed7c61a-0b8b-4bbd-e095-c050495496ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0 3394 impossible\n",
            " 1 4305 mr\n",
            " 2  813 bennet\n",
            " 3 3394 impossible\n",
            " 4 7078 when\n",
            " 5 3315 i\n",
            " 6  415 am\n",
            " 7 4436 not\n",
            " 8  239 acquainted\n",
            " 9 7148 with\n",
            "10 3215 him\n",
            "torch.Size([11, 7261])\n"
          ]
        }
      ],
      "source": [
        "word_t = torch.zeros(len(words_in_line), len(word2index_dict))\n",
        "for i, word in enumerate(words_in_line):\n",
        "    word_index = word2index_dict[word]\n",
        "    word_t[i][word_index] = 1\n",
        "    print('{:2} {:4} {}'.format(i, word_index, word))\n",
        "print(word_t.shape)\n",
        "#At this point, tensor represents one sentence of length 11 in an encoding space of size 7,261, the number of words in our dictionary."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}